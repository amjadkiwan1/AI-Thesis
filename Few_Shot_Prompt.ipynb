{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. The datasets have been saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Parsed_Evaluated_Questions_fixed.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Load the data from the identified sheet\n",
    "df = pd.read_excel(xls, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Identify criteria columns (grades for each criterion)\n",
    "criteria_columns = [col for col in df.columns if \"Grade\" in col and col != \"Average\"]\n",
    "\n",
    "# Filter high-scoring questions (average score >= 2.5)\n",
    "high_scoring_df = df[df[\"Average\"] >= 2.5]\n",
    "\n",
    "# Select one high-scoring question per criterion (highest score per column)\n",
    "high_scoring_questions = pd.DataFrame()\n",
    "\n",
    "for criterion in criteria_columns:\n",
    "    top_question = high_scoring_df.loc[high_scoring_df[criterion].idxmax()]\n",
    "    high_scoring_questions = pd.concat([high_scoring_questions, top_question.to_frame().T])\n",
    "\n",
    "# Remove duplicates (in case the same question is selected multiple times)\n",
    "high_scoring_questions = high_scoring_questions.drop_duplicates()\n",
    "\n",
    "# Create a low-scoring dataset (questions that were not selected)\n",
    "low_scoring_df = df.drop(high_scoring_questions.index)\n",
    "\n",
    "# Save the extracted datasets as CSV files\n",
    "high_scoring_questions.to_csv(\"High_Scoring_Questions.csv\", index=False)\n",
    "low_scoring_df.to_csv(\"Low_Scoring_Questions.csv\", index=False)\n",
    "\n",
    "print(\"Extraction complete. The datasets have been saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Relevance Grade</th>\n",
       "      <th>Relevance Reason</th>\n",
       "      <th>Curriculum Fit Grade</th>\n",
       "      <th>Curriculum Fit Reason</th>\n",
       "      <th>Accuracy Grade</th>\n",
       "      <th>Accuracy Reason</th>\n",
       "      <th>Clarity Grade</th>\n",
       "      <th>...</th>\n",
       "      <th>Fairness Reason</th>\n",
       "      <th>Avoiding Plagiarism Grade</th>\n",
       "      <th>Avoiding Plagiarism Reason</th>\n",
       "      <th>Novelty Grade</th>\n",
       "      <th>Novelty Reason</th>\n",
       "      <th>DOK Level Grade</th>\n",
       "      <th>DOK Level Reason</th>\n",
       "      <th>Bloom's Taxonomy Grade</th>\n",
       "      <th>Bloom's Taxonomy Reason</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What happens to the particles in the water whe...</td>\n",
       "      <td>['The particles move faster and spread out.',\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The question aligns well with learning outcome...</td>\n",
       "      <td>3</td>\n",
       "      <td>This question is appropriate for middle school...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question is accurate; boiling involves par...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>The question ensures fairness and accessibilit...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question is fairly standard and doesn't sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>The question lacks novelty or a fresh perspect...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The question reflects a basic recall level, wh...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question tests the 'Understand' level of B...</td>\n",
       "      <td>2.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Select the invertebrate.</td>\n",
       "      <td>['a butterfly' 'a dog' 'a bird'],</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The question correctly targets the identificat...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question is directly relevant to elementar...</td>\n",
       "      <td>2</td>\n",
       "      <td>The answer key points to the wrong choice ('1'...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>The question is fair and accessible to a diver...</td>\n",
       "      <td>3</td>\n",
       "      <td>The content appears fresh and unique, not a re...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question presents a common topic without i...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The question aligns with the Recall level, whi...</td>\n",
       "      <td>2</td>\n",
       "      <td>This question assesses the Remember level, foc...</td>\n",
       "      <td>2.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Select the mammals.</td>\n",
       "      <td>['a blue jay' 'a goldfish' 'a house cat' 'a ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The question aligns with learning outcomes rel...</td>\n",
       "      <td>3</td>\n",
       "      <td>Identifying mammals fits well within elementar...</td>\n",
       "      <td>2</td>\n",
       "      <td>The content is mostly accurate; however, the i...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>The question is fair and accessible to diverse...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question appears unique and not a rephrase...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question skillfully uses common animals to...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The question reflects recall, a lower DOK leve...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question aligns with the 'Remember' level ...</td>\n",
       "      <td>2.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Select the elementary substance.</td>\n",
       "      <td>['Aluminum' 'calcium hydroxide''silicon dioxid...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>The question aligns with standard learning out...</td>\n",
       "      <td>3</td>\n",
       "      <td>It is suitable for a high school science curri...</td>\n",
       "      <td>3</td>\n",
       "      <td>The content is accurate. Aluminum is indeed an...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>The question is fair and accessible, assuming ...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question appears unique and doesn't seem t...</td>\n",
       "      <td>2</td>\n",
       "      <td>While the question introduces important concep...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The question reflects the appropriate Depth of...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question requires students to \"Remember\" a...</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  \\\n",
       "8    What happens to the particles in the water whe...   \n",
       "10                            Select the invertebrate.   \n",
       "43                                 Select the mammals.   \n",
       "102                   Select the elementary substance.   \n",
       "\n",
       "                                               Choices Answer Relevance Grade  \\\n",
       "8    ['The particles move faster and spread out.',\\...      1               3   \n",
       "10                   ['a butterfly' 'a dog' 'a bird'],      1               3   \n",
       "43   ['a blue jay' 'a goldfish' 'a house cat' 'a ho...      1               3   \n",
       "102  ['Aluminum' 'calcium hydroxide''silicon dioxid...      0               3   \n",
       "\n",
       "                                      Relevance Reason Curriculum Fit Grade  \\\n",
       "8    The question aligns well with learning outcome...                    3   \n",
       "10   The question correctly targets the identificat...                    3   \n",
       "43   The question aligns with learning outcomes rel...                    3   \n",
       "102  The question aligns with standard learning out...                    3   \n",
       "\n",
       "                                 Curriculum Fit Reason Accuracy Grade  \\\n",
       "8    This question is appropriate for middle school...              3   \n",
       "10   The question is directly relevant to elementar...              2   \n",
       "43   Identifying mammals fits well within elementar...              2   \n",
       "102  It is suitable for a high school science curri...              3   \n",
       "\n",
       "                                       Accuracy Reason Clarity Grade  ...  \\\n",
       "8    The question is accurate; boiling involves par...             3  ...   \n",
       "10   The answer key points to the wrong choice ('1'...             3  ...   \n",
       "43   The content is mostly accurate; however, the i...             3  ...   \n",
       "102  The content is accurate. Aluminum is indeed an...             2  ...   \n",
       "\n",
       "                                       Fairness Reason  \\\n",
       "8    The question ensures fairness and accessibilit...   \n",
       "10   The question is fair and accessible to a diver...   \n",
       "43   The question is fair and accessible to diverse...   \n",
       "102  The question is fair and accessible, assuming ...   \n",
       "\n",
       "    Avoiding Plagiarism Grade  \\\n",
       "8                           2   \n",
       "10                          3   \n",
       "43                          3   \n",
       "102                         3   \n",
       "\n",
       "                            Avoiding Plagiarism Reason Novelty Grade  \\\n",
       "8    The question is fairly standard and doesn't sh...             1   \n",
       "10   The content appears fresh and unique, not a re...             2   \n",
       "43   The question appears unique and not a rephrase...             3   \n",
       "102  The question appears unique and doesn't seem t...             2   \n",
       "\n",
       "                                        Novelty Reason DOK Level Grade  \\\n",
       "8    The question lacks novelty or a fresh perspect...             2.0   \n",
       "10   The question presents a common topic without i...             2.0   \n",
       "43   The question skillfully uses common animals to...             2.0   \n",
       "102  While the question introduces important concep...             3.0   \n",
       "\n",
       "                                      DOK Level Reason Bloom's Taxonomy Grade  \\\n",
       "8    The question reflects a basic recall level, wh...                      2   \n",
       "10   The question aligns with the Recall level, whi...                      2   \n",
       "43   The question reflects recall, a lower DOK leve...                      2   \n",
       "102  The question reflects the appropriate Depth of...                      3   \n",
       "\n",
       "                               Bloom's Taxonomy Reason   Average  \n",
       "8    The question tests the 'Understand' level of B...  2.555556  \n",
       "10   This question assesses the Remember level, foc...  2.611111  \n",
       "43   The question aligns with the 'Remember' level ...  2.555556  \n",
       "102  The question requires students to \"Remember\" a...  2.666667  \n",
       "\n",
       "[4 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_scoring_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Relevance Grade</th>\n",
       "      <th>Relevance Reason</th>\n",
       "      <th>Curriculum Fit Grade</th>\n",
       "      <th>Curriculum Fit Reason</th>\n",
       "      <th>Accuracy Grade</th>\n",
       "      <th>Accuracy Reason</th>\n",
       "      <th>Clarity Grade</th>\n",
       "      <th>...</th>\n",
       "      <th>Fairness Reason</th>\n",
       "      <th>Avoiding Plagiarism Grade</th>\n",
       "      <th>Avoiding Plagiarism Reason</th>\n",
       "      <th>Novelty Grade</th>\n",
       "      <th>Novelty Reason</th>\n",
       "      <th>DOK Level Grade</th>\n",
       "      <th>DOK Level Reason</th>\n",
       "      <th>Bloom's Taxonomy Grade</th>\n",
       "      <th>Bloom's Taxonomy Reason</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the information provided</td>\n",
       "      <td>['When the salt is dissolved in water.' 'When ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The question lacks sufficient context, making ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Without specific subject or grade level contex...</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer choices lack context, causing uncer...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Without context, it's challenging to ensure fa...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question seems unique, though verifying it...</td>\n",
       "      <td>0</td>\n",
       "      <td>Due to missing context, there is no opportunit...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Insufficient context makes it impossible to de...</td>\n",
       "      <td>0</td>\n",
       "      <td>Without knowing the question's intent or conte...</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which of these questions can be investigated w...</td>\n",
       "      <td>['What is the average height of a tomato plant...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>The question partially aligns with investigati...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question is suitable for a middle-grade sc...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question is mostly accurate, but the phras...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>The question is accessible and fair to all lea...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question is somewhat generic, raising conc...</td>\n",
       "      <td>1</td>\n",
       "      <td>The question lacks novelty or a fresh perspect...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The question reflects the 'Strategic Thinking'...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question aligns with the 'Analyze' level o...</td>\n",
       "      <td>2.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the inherited trait that this rabbit has?</td>\n",
       "      <td>['The rabbit has white fur.' 'The rabbit has a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>The question is somewhat relevant to the genet...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question can fit a biology curriculum disc...</td>\n",
       "      <td>1</td>\n",
       "      <td>The question's wording is somewhat ambiguous. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>While the question is generally fair, the lack...</td>\n",
       "      <td>2</td>\n",
       "      <td>There is no evidence of direct copying, but th...</td>\n",
       "      <td>0</td>\n",
       "      <td>This question lacks any novel or unique approa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question targets a low Depth of Knowledge ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The question aligns with the 'Remember' level ...</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What happens to the sugar?\\n Choices: \\n['It i...</td>\n",
       "      <td>['It is a physical change.' 'It is a chemical ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The question is moderately aligned with common...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question fits within science curricula tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>The question is overly simplistic and ambiguou...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>The lack of context could make it confusing fo...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question appears to be original and not a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The question lacks novelty or a fresh perspect...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question lacks depth, as it does not engag...</td>\n",
       "      <td>1</td>\n",
       "      <td>The question aligns with the \"Remember\" level ...</td>\n",
       "      <td>2.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Select the plant.</td>\n",
       "      <td>['a cat' 'a rose bush' 'a tomato plant'],</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>While the question is related to identifying a...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question is suitable for early elementary ...</td>\n",
       "      <td>3</td>\n",
       "      <td>The information is accurate. A rose bush is in...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>The question is fair and accessible to diverse...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question could be a common pedagogical exa...</td>\n",
       "      <td>1</td>\n",
       "      <td>It lacks novelty, as identifying basic plants ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question is at a recall level of Depth of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>The question aligns with the 'Remember' level ...</td>\n",
       "      <td>2.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0                  Based on the information provided   \n",
       "1  Which of these questions can be investigated w...   \n",
       "2  What is the inherited trait that this rabbit has?   \n",
       "3  What happens to the sugar?\\n Choices: \\n['It i...   \n",
       "4                                  Select the plant.   \n",
       "\n",
       "                                             Choices Answer  Relevance Grade  \\\n",
       "0  ['When the salt is dissolved in water.' 'When ...      1                1   \n",
       "1  ['What is the average height of a tomato plant...      2                2   \n",
       "2  ['The rabbit has white fur.' 'The rabbit has a...      0                2   \n",
       "3  ['It is a physical change.' 'It is a chemical ...      1                2   \n",
       "4          ['a cat' 'a rose bush' 'a tomato plant'],      1                2   \n",
       "\n",
       "                                    Relevance Reason  Curriculum Fit Grade  \\\n",
       "0  The question lacks sufficient context, making ...                     0   \n",
       "1  The question partially aligns with investigati...                     2   \n",
       "2  The question is somewhat relevant to the genet...                     2   \n",
       "3  The question is moderately aligned with common...                     2   \n",
       "4  While the question is related to identifying a...                     2   \n",
       "\n",
       "                               Curriculum Fit Reason  Accuracy Grade  \\\n",
       "0  Without specific subject or grade level contex...               1   \n",
       "1  The question is suitable for a middle-grade sc...               2   \n",
       "2  The question can fit a biology curriculum disc...               1   \n",
       "3  The question fits within science curricula tha...               1   \n",
       "4  The question is suitable for early elementary ...               3   \n",
       "\n",
       "                                     Accuracy Reason  Clarity Grade  ...  \\\n",
       "0  The answer choices lack context, causing uncer...              0  ...   \n",
       "1  The question is mostly accurate, but the phras...              2  ...   \n",
       "2  The question's wording is somewhat ambiguous. ...              2  ...   \n",
       "3  The question is overly simplistic and ambiguou...              2  ...   \n",
       "4  The information is accurate. A rose bush is in...              3  ...   \n",
       "\n",
       "                                     Fairness Reason  \\\n",
       "0  Without context, it's challenging to ensure fa...   \n",
       "1  The question is accessible and fair to all lea...   \n",
       "2  While the question is generally fair, the lack...   \n",
       "3  The lack of context could make it confusing fo...   \n",
       "4  The question is fair and accessible to diverse...   \n",
       "\n",
       "   Avoiding Plagiarism Grade  \\\n",
       "0                          3   \n",
       "1                          2   \n",
       "2                          2   \n",
       "3                          3   \n",
       "4                          2   \n",
       "\n",
       "                          Avoiding Plagiarism Reason  Novelty Grade  \\\n",
       "0  The question seems unique, though verifying it...              0   \n",
       "1  The question is somewhat generic, raising conc...              1   \n",
       "2  There is no evidence of direct copying, but th...              0   \n",
       "3  The question appears to be original and not a ...              1   \n",
       "4  The question could be a common pedagogical exa...              1   \n",
       "\n",
       "                                      Novelty Reason  DOK Level Grade  \\\n",
       "0  Due to missing context, there is no opportunit...              0.0   \n",
       "1  The question lacks novelty or a fresh perspect...              2.0   \n",
       "2  This question lacks any novel or unique approa...              1.0   \n",
       "3  The question lacks novelty or a fresh perspect...              1.0   \n",
       "4  It lacks novelty, as identifying basic plants ...              1.0   \n",
       "\n",
       "                                    DOK Level Reason  Bloom's Taxonomy Grade  \\\n",
       "0  Insufficient context makes it impossible to de...                       0   \n",
       "1  The question reflects the 'Strategic Thinking'...                       2   \n",
       "2  The question targets a low Depth of Knowledge ...                       1   \n",
       "3  The question lacks depth, as it does not engag...                       1   \n",
       "4  The question is at a recall level of Depth of ...                       2   \n",
       "\n",
       "                             Bloom's Taxonomy Reason   Average  \n",
       "0  Without knowing the question's intent or conte...  1.333333  \n",
       "1  The question aligns with the 'Analyze' level o...  2.388889  \n",
       "2  The question aligns with the 'Remember' level ...  2.000000  \n",
       "3  The question aligns with the \"Remember\" level ...  2.055556  \n",
       "4  The question aligns with the 'Remember' level ...  2.444444  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_scoring_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Few_Shot_Prompt.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the correct dataset with evaluation results\n",
    "file_path = \"Parsed_Evaluated_Questions_fixed.xlsx\"  # Update if necessary\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Define weights for each criterion (same as before)\n",
    "weights = {\n",
    "    \"Relevance Grade\": 0.15,\n",
    "    \"Curriculum Fit Grade\": 0.12,\n",
    "    \"Accuracy Grade\": 0.12,\n",
    "    \"Clarity Grade\": 0.10,\n",
    "    \"Single Correct Answer Grade\": 0.15,\n",
    "    \"DOK Level Grade\": 0.10,\n",
    "    \"Bloom's Taxonomy Grade\": 0.08,\n",
    "    \"Bias-Free (Answer Choices) Grade\": 0.04,\n",
    "    \"Grammar and Syntax Grade\": 0.03,\n",
    "    \"Avoiding Common Errors Grade\": 0.02,\n",
    "    \"Readability Grade\": 0.03,\n",
    "    \"Plausibility Grade\": 0.03,\n",
    "    \"Balance (Answer Choices) Grade\": 0.03,\n",
    "    \"Bias-Free Content Grade\": 0.03,\n",
    "    \"Formatting Grade\": 0.02,\n",
    "    \"Fairness Grade\": 0.03,\n",
    "    \"Avoiding Plagiarism Grade\": 0.02,\n",
    "    \"Novelty Grade\": 0.02\n",
    "}\n",
    "\n",
    "# Normalize weights to ensure they sum to 1\n",
    "total_weight = sum(weights.values())\n",
    "weights = {k: v / total_weight for k, v in weights.items()}\n",
    "\n",
    "# Identify critical criteria (weights >= 0.10)\n",
    "critical_criteria = [k for k, v in weights.items() if v >= 0.10]\n",
    "\n",
    "# Calculate weighted average\n",
    "data['Weighted Average'] = sum(data[col] * weights[col] for col in weights.keys())\n",
    "\n",
    "# Define passing criteria\n",
    "passing_threshold = 1.5\n",
    "\n",
    "# Check for failure in critical criteria\n",
    "data['Fails Critical Criteria'] = data[critical_criteria].apply(lambda row: (row < passing_threshold).any(), axis=1)\n",
    "data['Passed'] = (~data['Fails Critical Criteria']) & (data['Weighted Average'] >= passing_threshold)\n",
    "\n",
    "# Extract new high-performing questions (weighted average > 2.5)\n",
    "high_scoring_questions = data[(data[\"Passed\"] == True) & (data[\"Weighted Average\"] > 2.5)]\n",
    "\n",
    "# Select the top 5 highest-performing questions based on Weighted Average\n",
    "top_5_high_performers = high_scoring_questions.nlargest(5, \"Weighted Average\")\n",
    "\n",
    "# Generate a new few-shot prompt with the top 5 high-performing questions\n",
    "few_shot_prompt = \"\"\"### Few-Shot Learning Prompt for Question Improvement\n",
    "\n",
    "The goal of this task is to improve low-scoring questions by using high-performing questions as a guide. Given metadata about a low-scoring question, regenerate a new question that aligns with the high-scoring examples.\n",
    "\n",
    "#### Example High-Performing Questions:\n",
    "\"\"\"\n",
    "\n",
    "# Include the top 5 high-performing questions as examples with concatenated reasons\n",
    "for idx, row in top_5_high_performers.iterrows():\n",
    "    concatenated_reasons = \" \".join(str(row[col]) for col in reason_columns if pd.notna(row[col]))\n",
    "\n",
    "    few_shot_prompt += f\"\"\"\n",
    "**Question:** {row['Question']}\n",
    "**Choices:** {row['Choices']}\n",
    "**Answer:** {row['Answer']}\n",
    "**Explanation:** {concatenated_reasons}\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt += \"\"\"\\n\n",
    "#### Example Low-Scoring Questions (Avoid these types of questions):\n",
    "\"\"\"\n",
    "\n",
    "# Select three representative low-scoring questions as examples to avoid\n",
    "num_low_scoring_examples = min(3, len(data[data[\"Passed\"] == False]))  # Ensure we don't exceed available data\n",
    "sample_low_scoring_rows = data[data[\"Passed\"] == False].iloc[:num_low_scoring_examples]\n",
    "\n",
    "# Include three low-scoring questions as examples of what to avoid\n",
    "for idx, row in sample_low_scoring_rows.iterrows():\n",
    "    concatenated_reasons_low = \" \".join(str(row[col]) for col in reason_columns if pd.notna(row[col]))\n",
    "\n",
    "    few_shot_prompt += f\"\"\"\n",
    "**Question:** {row['Question']}\n",
    "**Choices:** {row['Choices']}\n",
    "**Answer:** {row['Answer']}\n",
    "**Issues:** {concatenated_reasons_low}\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt += \"\"\"\\n\n",
    "#### Now, generate a new question based on the metadata and the high-scoring examples above while avoiding the mistakes seen in the low-scoring examples:\n",
    "\"\"\"\n",
    "\n",
    "# Save the updated prompt to a text file\n",
    "prompt_file_path = \"Few_Shot_Prompt.txt\"\n",
    "with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(few_shot_prompt)\n",
    "\n",
    "# Provide the file for download\n",
    "prompt_file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
