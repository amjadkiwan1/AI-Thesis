{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to Evaluated_Questions_few_shot.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "# Load the rubric and generated question data\n",
    "rubric_data = pd.read_excel('MCQ_Evaluation_Rubric.xlsx')\n",
    "generations_data = pd.read_excel('formatted_generated_test_results_few_shot.xlsx')\n",
    "\n",
    "# Define a function to evaluate a question using the rubric\n",
    "def evaluate_question(question, choices, answer, rubric):\n",
    "    \"\"\"\n",
    "    Evaluates a question against a rubric using an LLM.\n",
    "\n",
    "    Args:\n",
    "        question (str): The generated question text.\n",
    "        choices (list): List of answer choices.\n",
    "        answer (str): The correct answer.\n",
    "        rubric (pd.DataFrame): The rubric dataframe.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw evaluation response from the model.\n",
    "    \"\"\"\n",
    "    rubric_text = \"\\n\".join(\n",
    "        [f\"{row['Criteria']}: {row['Explanation']}\" for _, row in rubric.iterrows()]\n",
    "    )\n",
    "\n",
    "    prompt = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert evaluator for educational questions. Be meticulous, accurate, and thorough in your evaluations. Ensure that grades (0-3) are assigned with strict adherence to the rubric's explanations. All grades across the range must be used appropriately, and your evaluations should include clear, unbiased reasoning.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                You are an expert evaluator assessing a multiple-choice question based on the following rubric:\n",
    "                \n",
    "                Rubric Criteria:\n",
    "                {rubric_text}\n",
    "\n",
    "                Question: {question}\n",
    "                Choices: {choices}\n",
    "                Answer: {answer}\n",
    "\n",
    "                Provide an evaluation for each of the 16 criteria using the full grade range (0-3). For each criterion, provide:\n",
    "                - Criterion name\n",
    "                - Grade (0-3)\n",
    "                - Detailed reason for the grade\n",
    "\n",
    "                Calculate the total percentage score as the average of all grades (e.g., Total Percentage: 85.4%).\n",
    "\n",
    "                Output Format:\n",
    "                Criterion: <Criterion Name> | Grade: <Grade (0-3)> | Reason: <Reason>\n",
    "                ...\n",
    "                Total Percentage: <Percentage>\n",
    "                Ensure the response strictly follows this format for easier parsing.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Use the OpenAI client to evaluate\n",
    "    response = client.chat.completions.create(**prompt, max_tokens=2000)\n",
    "    evaluation_text = response.choices[0].message.content\n",
    "\n",
    "    return evaluation_text\n",
    "\n",
    "# Main loop for evaluations\n",
    "evaluations = []\n",
    "for _, row in generations_data.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    choices = row[\"choices\"]\n",
    "    answer = row[\"answer\"]\n",
    "\n",
    "    evaluation_text = evaluate_question(question, choices, answer, rubric_data)\n",
    "    evaluation_data = {\n",
    "        \"Question\": question,\n",
    "        \"Choices\": choices,\n",
    "        \"Answer\": answer,\n",
    "        \"Raw Evaluation\": evaluation_text\n",
    "    }\n",
    "    evaluations.append(evaluation_data)\n",
    "\n",
    "# Convert evaluations to a DataFrame\n",
    "evaluation_results = pd.DataFrame(evaluations)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "evaluation_results.to_excel('Evaluated_Questions_few_shot.xlsx', index=False)\n",
    "\n",
    "print(\"Evaluation complete. Results saved to Evaluated_Questions_few_shot.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
